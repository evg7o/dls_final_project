{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fd85d9-3914-4b1d-919c-29d58ebe8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем необходимые библиотеки \n",
    "# Библиотека для работы с функциями операционной системы\n",
    "import os\n",
    "\n",
    "# Даталоадер\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Загрузка изображений \n",
    "from torchvision.datasets import ImageFolder\n",
    "# Функции трансформации изображений, сохранения и компоновки \n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Общего применения \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', font_scale=1.2)\n",
    "import itertools\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "# import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11c73ac-e916-46f7-a130-a12f135adc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060\n",
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# Смотрим видеокарты установленные в системе \n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806dd195-2552-4ebb-98cd-7f37a93eb591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Определяемся с устройством \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9f2507-86f5-48c1-8317-ff3c793c8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры для обработки изображения\n",
    "image_size = 256\n",
    "# Константы нормализации изображения \n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86fde84-9eaf-4cfa-bf7b-54c5f649b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Денормализация для вывода изображения, в генераторе на выходе гиперболтческий тангенс \n",
    "# Денормализация отобразит его выход [-1,1] в [0,1] для вывода\n",
    "def denorm(img_tensors):\n",
    "    return torch.clamp(img_tensors * stats[1][0] + stats[0][0],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8146dff7-755c-4351-b9e9-6420876becd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция вывода изображения\n",
    "def show_images(images, nmax=4, title=''):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=2).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ecf0cb-57d4-4faf-9465-4c7b0b132636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка состояния модели и птимизатора\n",
    "def load_state(path, model, optimizer):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df546f7-d337-405d-8498-b1863ab35be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок Реснет \n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.resnet  = nn.Sequential(\n",
    "               \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, bias = True),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True), \n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, bias = True),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1197fb1-3e8e-4704-b7d0-1e2176969d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генератор\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        # базовое количество карт\n",
    "        self.chanel = 64\n",
    "        \n",
    "        # Сеть энкодера  \n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.chanel, kernel_size=7, stride=1, padding=0, bias = True),\n",
    "            nn.ReLU(inplace=True)  \n",
    "        ) #256 -> 256\n",
    "        \n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.chanel, out_channels=self.chanel*2, kernel_size=4, stride=2, padding=1, bias = True),\n",
    "            nn.InstanceNorm2d(self.chanel*2),\n",
    "            nn.ReLU(inplace=True) \n",
    "        ) #256 -> 128 \n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.chanel*2, out_channels=self.chanel*4, kernel_size=4, stride=2, padding=1, bias = True),\n",
    "            nn.InstanceNorm2d(self.chanel*4),\n",
    "            nn.ReLU(inplace=True)    \n",
    "        ) #128 -> 64\n",
    "        \n",
    "      \n",
    "        self.resnet1 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet2 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet3 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet4 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet5 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet6 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet7 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet8 = ResNetBlock(self.chanel*4)\n",
    "        self.resnet9 = ResNetBlock(self.chanel*4)\n",
    "        \n",
    "        \n",
    "         # Сеть декодера \n",
    "        \n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=self.chanel*4, out_channels=self.chanel*2, kernel_size=4, stride=2, padding=1, bias = True),\n",
    "            nn.InstanceNorm2d(self.chanel*2),\n",
    "            nn.ReLU(inplace=True)  \n",
    "       \n",
    "        ) # 64 -> 128\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=self.chanel*2, out_channels=self.chanel, kernel_size=4, stride=2, padding=1, bias = True),\n",
    "            nn.InstanceNorm2d(self.chanel),\n",
    "            nn.ReLU(inplace=True)  \n",
    "        ) # 128 -> 256\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=self.chanel, out_channels=3, kernel_size=7, stride=1, padding=0, bias = True),  \n",
    "            nn.Tanh()     \n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input(x)   # 256x256\n",
    "        x = self.down1(x)   # 128x128\n",
    "        x = self.down2(x)   # 64x64\n",
    "      \n",
    "        x = self.resnet1(x)\n",
    "        x = self.resnet2(x)\n",
    "        x = self.resnet3(x)\n",
    "        x = self.resnet4(x)\n",
    "        x = self.resnet5(x)\n",
    "        x = self.resnet6(x)\n",
    "        x = self.resnet7(x)\n",
    "        x = self.resnet8(x)\n",
    "        x = self.resnet9(x) # 64x64\n",
    "        \n",
    "        \n",
    "        x = self.up1(x)  # 128x128\n",
    "        x = self.up2(x)  # 256x256\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae75ab4f-9040-4786-a7e4-db4fbca08465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем генераторы  и тестируем \n",
    "generator_AB = GeneratorResNet()\n",
    "generator_BA = GeneratorResNet()\n",
    "generator_AB = generator_AB.to(device)\n",
    "generator_BA = generator_BA.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cd182a-b86c-4d70-b29d-44673d3745ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_g = torch.optim.Adam(itertools.chain(generator_AB.parameters(), generator_BA.parameters()), lr=2e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c45015a1-ab6f-4c1f-8c2f-f1b3a90477ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "load_state('/home/master/dls/CycleGAN_foto2maps_pix_idn_g_BA_epoch_499.tsm', generator_BA, opt_g)\n",
    "load_state('/home/master/dls/CycleGAN_foto2maps_pix_idn_g_AB_epoch_499.tsm', generator_AB, opt_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7121f674-9167-47cf-b578-fc1cf688599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705a6692-3fbc-4c7e-a755-9e6fc05f7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b45945-133a-461e-a930-32de75c9cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = telebot.TeleBot('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85c60f28-1ee4-4887-badc-f42425ac4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bot.message_handler(content_types=['photo'])\n",
    "def photo(message):\n",
    "    print('message.photo =', message.photo)\n",
    "    fileID = message.photo[-1].file_id\n",
    "    print('fileID =', fileID)\n",
    "    file_info = bot.get_file(fileID)\n",
    "    print('file.file_path =', file_info.file_path)\n",
    "    downloaded_file = bot.download_file(file_info.file_path)\n",
    "    with open(\"image.jpg\", 'wb') as new_file:\n",
    "        new_file.write(downloaded_file)\n",
    "        test_image = io.read_image('/home/master/dls/image.jpg')\n",
    "    x1 = test_image.shape[1]\n",
    "    x2 = test_image.shape[2]\n",
    "    test_image = test_image/255\n",
    "    test_image =  tt.Normalize(*stats)(test_image)\n",
    "    test_image = tt.Resize((image_size, image_size))(test_image)\n",
    "    with torch.no_grad():\n",
    "        test_result  = generator_BA(test_image.to(device)).to('cpu')\n",
    "    test_result = tt.Resize((x1, x2),  interpolation=tt.InterpolationMode.BICUBIC)(test_result)\n",
    "    plt.imsave('/home/master/dls/image_out.jpg', denorm(test_result.detach()).permute(1, 2, 0).numpy())\n",
    "    print('Transform: OK')\n",
    "    file = open('/home/master/dls/image_out.jpg', 'rb')\n",
    "    bot.send_photo(message.chat.id, file)\n",
    "    print('Send: OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8958077-93bb-4ceb-a0d3-09332a6776a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message.photo = [<telebot.types.PhotoSize object at 0x7eff493d1400>, <telebot.types.PhotoSize object at 0x7eff493d1580>, <telebot.types.PhotoSize object at 0x7eff493d16a0>, <telebot.types.PhotoSize object at 0x7eff493d18b0>]\n",
      "fileID = AgACAgIAAxkBAAO5ZKUPE3cJ303b6OckLK9CXvdBCMIAAijGMRsGfylJFUiYXHb3Mz4BAAMCAAN5AAMvBA\n",
      "file.file_path = photos/file_72.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform: OK\n",
      "Send: OK\n"
     ]
    }
   ],
   "source": [
    "bot.polling(none_stop=True, interval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e4ceb-0069-40d0-ac08-4a9d2af27945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
